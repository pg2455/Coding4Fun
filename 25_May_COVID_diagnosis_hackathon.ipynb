{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "25 May COVID-diagnosis-hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pg2455/Coding4Fun/blob/master/25_May_COVID_diagnosis_hackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFzda9am9vnj"
      },
      "source": [
        "# Covid-19 diagnosis using symptoms\n",
        "\n",
        "[Prateek Gupta](https://www.pgupta.info) \n",
        "\n",
        "2020-05-20\n",
        "\n",
        "**Abstract:** This notebook is a tutorial on building a typical machine learning classifier. \n",
        "The process of doing so spans steps ranging from data wrangling to model selection. \n",
        "With the help of each of these steps we hope to make the reader familiar with challenges involved in building a machine learning system. \n",
        "\n",
        "# Question\n",
        "\n",
        "In this hackathon, we want to build a machine learning model to predict COVID-19 infections from symptoms.\n",
        "It has several applications, for example, triaging patients to be attended to by a doctor or nurse, recommending self-isolation through contact tracing apps. \n",
        "\n",
        "Zoabi et al. [[1]](https://www.nature.com/articles/s41746-020-00372-6) builds a decision tree classifier using the publicly available data reported by the Israeli Ministry of Health.\n",
        "The paper itself dicsusses the various challenges encountered in deploying such a model. \n",
        "It is encouraged to read the paper and learn the challeges and ways to overcome them. \n",
        "\n",
        "However, in this hackathon, we will use their dataset and make the participant familiar with a typical pipeline of building a machine learning system.\n",
        "\n",
        "[1] [Zoabi, Y., Deri-Rozov, S. & Shomron, N. Machine learning-based prediction of COVID-19 diagnosis based on symptoms. npj Digit. Med. 4, 3 (2021).]((https://www.nature.com/articles/s41746-020-00372-6))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfQWCQDjAKN0"
      },
      "source": [
        "# Setup the workspace\n",
        "\n",
        "We will clone their Git repository to to use their dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yguOIEDX-YbD",
        "outputId": "b883c322-0614-401e-ccc4-9688938ad1af"
      },
      "source": [
        "!git clone https://github.com/nshomron/covidpred.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'covidpred'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 35 (delta 16), reused 10 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (35/35), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35jfRmroICGw"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv6FR_gOAV16"
      },
      "source": [
        "# Data \n",
        "\n",
        "Let's check how the data looks like and how various features are encoded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7tANpd5-tqj",
        "outputId": "98096b64-c733-4c43-8cbe-4d3f5abcea3f"
      },
      "source": [
        "df = pd.read_csv('covidpred/data/corona_tested_individuals_ver_006.english.csv.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1,2,3,4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXaLQdbBAElY",
        "outputId": "3501d509-7250-496e-bb59-cfc5d9249d81"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['test_date', 'cough', 'fever', 'sore_throat', 'shortness_of_breath',\n",
              "       'head_ache', 'corona_result', 'age_60_and_above', 'gender',\n",
              "       'test_indication'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWHY4TLAAhpJ"
      },
      "source": [
        "Great! These are the features used in the paper for their prediction task. The authors also list these features in the [README.md of their Github repo](https://github.com/nshomron/covidpred). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtPMd2qGn72"
      },
      "source": [
        "Pandas read columns as string, so we need to convert them to the proper format before we can operate on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi7esn5RGdC8"
      },
      "source": [
        "df['test_date'] = pd.to_datetime(df['test_date'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdb3D7Y5BdTt",
        "outputId": "75a9cc98-8a8a-4764-a7df-208cc85202d9"
      },
      "source": [
        "print(\"Start date:\", min(df['test_date']))\n",
        "print(\"End date:\", max(df['test_date']))\n",
        "\n",
        "n_days = (max(df['test_date']) - min(df['test_date'])).days\n",
        "print(\"# of days: \", n_days)\n",
        "\n",
        "n_obs =  df.shape[0]\n",
        "print(\"# of observations:\", df.shape[0])\n",
        "print(\"# of features:\", df.shape[1])\n",
        "\n",
        "pos_cases = sum(df['corona_result'] == \"positive\")\n",
        "print(\"# of positively diagnosed cases: {0} ({1: 2.2f}%)\".format(pos_cases, 100*pos_cases / n_obs))\n",
        "\n",
        "neg_cases = sum(df['corona_result'] == \"negative\")\n",
        "print(\"# of negatively diagnosed cases: {0} ({1: 2.2f}%)\".format(neg_cases, 100 * neg_cases / n_obs))\n",
        "\n",
        "other_cases = sum(df['corona_result'] == \"other\") # possibly not confirmed\n",
        "print(\"# of other cases (possibly, not confirmed): {0} ({1: 2.2f}%)\".format(other_cases, 100 * other_cases / n_obs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start date: 2020-03-11 00:00:00\n",
            "End date: 2020-04-30 00:00:00\n",
            "# of days:  50\n",
            "# of observations: 278848\n",
            "# of features: 10\n",
            "# of positively diagnosed cases: 14729 ( 5.28%)\n",
            "# of negatively diagnosed cases: 260227 ( 93.32%)\n",
            "# of other cases (possibly, not confirmed): 3892 ( 1.40%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7guFsw8K-Cvy"
      },
      "source": [
        "Since we do not have any information on what happened to \"other\" cases, we will exclude them from our exercise. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-2eeGvK-Bxv"
      },
      "source": [
        "df = df[df['corona_result'].isin(['positive', 'negative'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hG2lfabKr1e"
      },
      "source": [
        "To build out predictor, we will be splitting our dataset into **training,  validation, and test sets**. \n",
        "A model is trained on the training dataset while the hyperparameters are tuned on the validation dataset. \n",
        "Finally, a test dataset is used to report final model's performance metrics. \n",
        "\n",
        "Since we have a time dependent dataset we will split our training and test dataset based on time. \n",
        "Thus, we find the date before which 600% of observations are present, and use that as our training dataset.\n",
        "We will use next 20% of the dataset as our validation dataset, and finally, the remaining 20% will be used as a test dataset. \n",
        "Thus, we use 60-20-20 split.\n",
        "\n",
        "The authors use 63%-23% training-test split, and a further split of training into train-valid dataset using 80-20% split.\n",
        "There is no prescribed formula on how to do this split. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dEh77THGW6"
      },
      "source": [
        "date_counts = df.groupby(['test_date']).count()['gender'] # take count of any column. They will all be same.\n",
        "date_counts = date_counts.sort_index()\n",
        "cum_counts = date_counts.cumsum()\n",
        "cdf = cum_counts / n_obs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvTrB-PsMrOv",
        "outputId": "f46dd3b9-4a37-4c62-de4c-d0126ba4d511"
      },
      "source": [
        "max_training_date = cdf[cdf < 0.60].index.max()\n",
        "training_data = df[df['test_date'] <= max_training_date]\n",
        "\n",
        "min_test_date = cdf[cdf > 0.80].index.min()\n",
        "test_data = df[df['test_date'] >= min_test_date]\n",
        "\n",
        "valid_data = df[(max_training_date < df['test_date']) & (df['test_date'] < min_test_date)]\n",
        "\n",
        "print(\"# of observations in training dataset\", training_data.shape[0])\n",
        "print(\"# of observations in validation dataset\", valid_data.shape[0])\n",
        "print(\"# of observations in test dataset\", test_data.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of observations in training dataset 160717\n",
            "# of observations in validation dataset 53172\n",
            "# of observations in test dataset 61067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-xJrSsa5_Lz"
      },
      "source": [
        "**NOTE:**\n",
        "It is **extremely important** that you do not use the test dataset in the model building phase. \n",
        "While building models, it will be required to tune the hyperparameter, adjust assumptions, modify features, etc. \n",
        "This should be done on the validation dataset. \n",
        "After several such iterations on the validation dataset, you will pick a model with the best performace as your final model. \n",
        "\n",
        "A test dataset is used to measure the final model's performance, which is a proxy for how it will perform (or generalize) in real life. \n",
        "Thus, to have a proper measure of model's genearalization, test dataset should not be part of your model building process. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W86CzZGp_rOd"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "In this section, we will see the general statistics of features.\n",
        "In doing so, we will encounter inconsistencies in the data and address them accordingly. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx1mMFLQEMdS",
        "outputId": "b29c246d-3d7e-4a33-8d50-9ed5afa2fdcc"
      },
      "source": [
        "# We want to predict 'corona_result'. \n",
        "# We will not use \"test_date\" as a feature. \n",
        "# So we narrow down the input features to this list \n",
        "INPUT_FEATURES = ['cough', 'fever', 'sore_throat', 'shortness_of_breath', 'head_ache', 'age_60_and_above', 'gender', 'test_indication']\n",
        "TARGET_COLUMN = 'corona_result'\n",
        "\n",
        "for col in INPUT_FEATURES:\n",
        "  print(\"*\"*25, f\" {col} \", \"*\"*25)\n",
        "  print(training_data[col].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*************************  cough  *************************\n",
            "0       119702\n",
            "1        25547\n",
            "0        12039\n",
            "1         3177\n",
            "None       252\n",
            "Name: cough, dtype: int64\n",
            "*************************  fever  *************************\n",
            "0       131874\n",
            "0        13768\n",
            "1        13375\n",
            "1         1448\n",
            "None       252\n",
            "Name: fever, dtype: int64\n",
            "*************************  sore_throat  *************************\n",
            "0       142874\n",
            "0        15973\n",
            "1         1435\n",
            "1          434\n",
            "None         1\n",
            "Name: sore_throat, dtype: int64\n",
            "*************************  shortness_of_breath  *************************\n",
            "0       143279\n",
            "0        15906\n",
            "1         1030\n",
            "1          501\n",
            "None         1\n",
            "Name: shortness_of_breath, dtype: int64\n",
            "*************************  head_ache  *************************\n",
            "0       142294\n",
            "0        16064\n",
            "1         2015\n",
            "1          343\n",
            "None         1\n",
            "Name: head_ache, dtype: int64\n",
            "*************************  age_60_and_above  *************************\n",
            "No      123867\n",
            "Yes      25425\n",
            "None     11425\n",
            "Name: age_60_and_above, dtype: int64\n",
            "*************************  gender  *************************\n",
            "male      72598\n",
            "female    70950\n",
            "None      17169\n",
            "Name: gender, dtype: int64\n",
            "*************************  test_indication  *************************\n",
            "Other                     135059\n",
            "Abroad                     17162\n",
            "Contact with confirmed      8496\n",
            "Name: test_indication, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-bVRyzjR-dU"
      },
      "source": [
        "Here are the questions to guide you through the process of exploring data \n",
        "\n",
        "1.   Think about possible biases and limitations of this dataset\n",
        "2.   What is the format of feature values? Are there any inconsistencies? If so, how would you make them cosistent?\n",
        "3.   What is the statistics of these feature values? How many symptoms are reported or not?\n",
        "4.   Which symptoms have a reporting bias, i.e., likely to be reported when the patient is COVID positive? \n",
        "5.   How will the symptoms with reporting bias affect the model’s performance?\n",
        "6.   Visualization: Draw the bar graph of features grouped by the target class? \n",
        "7.   How does the bar graph of the symptoms with reporting bias looks like?\n",
        "8.   Determine if we have a class imbalance in the dataset? If so, what do you reckon will be the downstream challenges in evaluating the model? How will you overcome those challenges?\n",
        "9.   What does \"None\" value mean for feature? Should we include these features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQRohLR0R8wd"
      },
      "source": [
        "\n",
        "**GOOD PRACTICE**: To make your plots accessible to everyone, it is always a good idea to use colorblind-friendly palette for your plots. Check out [this](https://medium.com/cafe-pixo/inclusive-color-palettes-for-the-web-bbfe8cf2410e) for such a palette."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLRMrojV__Se"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "In this section, we will transform the features that models can operate upon. Note that this transformation doesn't have to be unique. \n",
        "It is very much dependent on the type of model you are building. \n",
        "\n",
        "Here is the list of questions to guide your feature engineering task \n",
        "\n",
        "1.   How will you represent the features in numerical format that can be accessible by model? \n",
        "2.   Are there any redundancies in your feature representation?\n",
        "3.   How will you represent targets in a format accessible to the model?\n",
        "\n",
        "Check out [`sklearn`'s preprocessing library](https://scikit-learn.org/stable/modules/preprocessing.html) for easy-to-use functions to do this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGLQMUmWWKch"
      },
      "source": [
        "def preprocess(data, encoder, lb):\n",
        "  \"\"\"\n",
        "  Transforms `data` into format required for model building\n",
        "\n",
        "  Args:\n",
        "    data (pd.DataFrame): dataframe with columns `INPUT_FEAUTURES` and `TARGET_COLUMN`\n",
        "    encoder (sklearn.preprocessing.OneHotEncoder): A fitted OneHotEncoder to be used to transform `INPUT_FEATURES`\n",
        "    lb (sklearn.preprocessing.LabelBinarizer): A fitted LabelBinarizer to be used to transform `TARGET_COLUMN`\n",
        "  \n",
        "  Returns:\n",
        "    model_input (np.array): each row is an observation, columns are one-hot encoded features of `INPUT_FEATURES`\n",
        "    model_target (np.array): 1D array with 1 where `TARGET_COLUMN` is \"positive\" and 0 otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  ###### YOUR CODE ########\n",
        "  return model_input, model_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N6PYvo8NUuE"
      },
      "source": [
        "# Model building\n",
        "\n",
        "In this section, we will build various classifiers using `sklearn`. You do not have to restrict yourself to `sklearn`. Please feel free to use any other library.\n",
        "\n",
        "**TRY:**  Try various classifiers that you have learned so far.\n",
        "Here is the list of models to try :\n",
        "\n",
        "*  Logistic Regssion: [User Guide](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression). [API](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "*   Decision Trees: [User Guide](https://scikit-learn.org/stable/modules/tree.html#classification). [API](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). You can head down in the User Guide to [other Tree algorithms](https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart) if you fancy. \n",
        "*   Categorical Naive Bayes.[User Guide](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes). [API](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB)\n",
        "*   Linear Discriminant Analysis. [User Guide](https://scikit-learn.org/stable/modules/lda_qda.html#). [API](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)\n",
        "*   Quadratic Discriminant Analysis. [User Guide](https://scikit-learn.org/stable/modules/lda_qda.html#). [API](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)\n",
        "*   Support Vector Machines. [User Guide](https://scikit-learn.org/stable/modules/svm.html#classification). [API](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
        "*   Nearest neighbors classification. [User Guide](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification). [API](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
        "*   Neural networks - Multi-layer Perceptron (MLP). [User Guide](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron). [API](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZSS0TyANfG1"
      },
      "source": [
        "# Evaluate Model\n",
        "\n",
        "In this section, we will evaluate our model's performance on the validation dataset. \n",
        "\n",
        "Here are the list of questions to think about while deciding how to evaluate your model - \n",
        "*   Is accuracy the right metric to evaluate the model? Are inaccuracies correctly penalized in the accuracy metric?\n",
        "*  Would you think that the cost of false negative is more than the false positive? Is it dependent on the application?\n",
        "*  Which metric will minimize false negatives and false positives?\n",
        "*   Which dataset should you chose to evaluate the model? Validation or Test?\n",
        "What other metric is relevant in our context?  \n",
        "\n",
        "For benchmarking everyone’s results we will stick to ROC AUC score as a metric. \n",
        "There are standard functions to compute these scores in `sklearn`, so we will use them. \n",
        "Specifically, we will be using [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) and [`plot_roc_curve`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TGfy0kYV7pM"
      },
      "source": [
        "def evaluate(model, X, Y):\n",
        "  \"\"\"\n",
        "  Returns the AUC-ROC for `model` as evaluated on (X, Y)\n",
        "\n",
        "  Args:\n",
        "    model (): Any model that has a function predict_proba and returns probability for each row in `X`.\n",
        "    X (np.array): Input to the model containing feature values\n",
        "    Y (np.array): 1D array containing true class i.e. 0 or 1\n",
        "  \n",
        "  Returns:\n",
        "    (float): AUC ROC for the model\n",
        "  \"\"\"\n",
        "\n",
        "  # For example - \n",
        "  # y_score = model.predict_proba(X) # (n_samples, n_clases) with each value being the probability of being in that class\n",
        "  # return roc_auc_score(y_true=Y, y_score=y_score[:, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n0jr-EekESj"
      },
      "source": [
        "# Hyperparameter Search\n",
        "\n",
        "In this section we will be searching for the best parameters to build our models. \n",
        "This is where we will use our validation dataset. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqSd5Bo3chPT"
      },
      "source": [
        "Hyperparameter search can become messy if you have lots of paramters. \n",
        "A brute force method to do such a search will be to do a grid search to fit tons of models. \n",
        "Thus, a smarter way to do hyperparameter search has been the subject of research. \n",
        "\n",
        "**TRY:** If interested, read [here](https://scikit-learn.org/stable/modules/grid_search.html) for more details and incorporate some of those ideas in the model building process. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkB6mMuSkkmN"
      },
      "source": [
        "# Report your results\n",
        "\n",
        "**NOTE:** You should use the test dataset only when you are done with hyperparameter search on your model. \n",
        "This is because the test dataset is not involved in the model building process, thereby making sure that the performance evaluation on the test dataset measures how well the proposed model is able to generalize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxqeBPIMko9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36d385f-50bf-4abe-c447-27861025a9f0"
      },
      "source": [
        "X_test, Y_test = preprocess(test_data, encoder, lb)\n",
        "auc_score = evaluate(best_model, X_test, Y_test)\n",
        "print(f\"AUC-ROC on the test dataset:{auc_score}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC-ROC on the test dataset:0.8114507841261618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnEnKthyBGz"
      },
      "source": [
        "Is this the end?\n",
        "\n",
        "What do you need to do to make the model practically applicable? How would you use this model in the real life?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5sO-B1-BnFz"
      },
      "source": [
        "# Collaborate with Ensemble \n",
        "\n",
        "You can combine various models to form an ensemble model. \n",
        "There are various ways to combine these models. \n",
        "All of them serve a particular purpose (e.g. reducing variance, increasing accuracy. etc.). \n",
        "\n",
        "The simplest example of an ensemble is to combine the constituent models via voting. This can be done in two ways:\n",
        "*  soft voting - likelihood of the input belonging to a class is the mean of the likelihood predicted by the constituent models, \n",
        "*  hard-voting - likelihood of the input belonging to a class is determined by frequency of the constutuent models predicting that class for the input.\n",
        "\n",
        "Thus, if you are working in a team, let each member try out different models. At the end, combine your models and make an ensemble model. \n",
        "\n",
        "Here is the [User Guide](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier) and [API](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) for making such an ensemble model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awguX_CZs17n"
      },
      "source": [
        "# Extras: Class imbalance\n",
        "\n",
        "**TRY:** \n",
        " Why not try resampling techniques to optimize for ROC AUC?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRQOtp3sNoKq"
      },
      "source": [
        "# Extras: Dimensionality Reduction\n",
        "\n",
        "**TRY:** You can try dimensionality reduction from 16 dimensions to just 2 dimensions and visualize 2D plot with just two categories - \"positive\" and \"negative\". \n",
        "To do this, try various dimension reduction techniques, for example, [LDA](https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-lda-dimensionality-reduction), [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA), [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-B7PKHpewgz"
      },
      "source": [
        "# Poster template \n",
        "\n",
        "It is often the most difficult task to communicate the project's finding concisely in 1 minute or in a 1 slide. \n",
        "Therefore, our suggestion will be to touch upon the following points in your poster -  \n",
        "\n",
        "1. Briefly define the problem\n",
        "2. Briefly describe the dataset \n",
        "3. What did you learn about various models/techniques/etc.? e.g. \n",
        "4. What's the auc score of your final model did you get?\n",
        "\n",
        "If you want to learn what matters in building such posters, check out this great [YouTube video](https://www.youtube.com/watch?v=1RwJbhkCA58).\n",
        "Specifically, we should try and avoid posters with a lot of stuff to avoid cognitive overload. \n",
        "The templates suggested in the video can be found [here](https://osf.io/ef53g/). \n",
        "Note these are for academic papers, however, you can follow a similar ideology to concizsely display your work. "
      ]
    }
  ]
}